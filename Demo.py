# -*- coding: utf8 -*-
import csv
import re
from pyvi import ViTokenizer
def remove_noise(st):
    specials = r"[->\[\]\'\\\n\.\.\.\?\"\â€œ\â€\-\,\,>>():;!@#$%^&*?~/`<>{}=+-_]\ufeff"
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    st = str(st).lower()
    st = re.sub(emoji_pattern, ' ', st)
    st = re.sub(specials, ' ', st)
    st = re.sub('\s.\s', ' ', st)
    st = re.sub('\s+\s', ' ', st)
    st = st.strip()
    return st
viet_list = []
with open('TXT_Viet.txt', 'r', encoding='utf8') as file:
    for row in file:
        row = remove_noise(row)
        row = row.replace(' ', '_')
        viet_list.append(row)
viet_list = list(set(viet_list))
dauvaodautien=[]
with open('dauvaodautien.csv', 'r', encoding='utf8') as csvFile:
    for row in csvFile:
        dauvaodautien.append(row)
csvFile.close()
dauvaodautien = [i.replace('\n', "") for i in dauvaodautien]
dauvaodautien = [i for i in dauvaodautien if i != ""]
replace_list = {
            #Quy cÃ¡c icon vá» 2 loáº¡i emoj: TÃ­ch cá»±c hoáº·c tiÃªu cá»±c
            "ğŸ‘¹": " ráº¥t chÃ¡n ", "ğŸ‘»": " ráº¥t tá»‘t ", "ğŸ’ƒ": " ráº¥t tá»‘t ",'ğŸ¤™': ' ráº¥t tá»‘t ', 'ğŸ‘': ' ráº¥t tá»‘t ',
            "ğŸ’„": " ráº¥t tá»‘t ", "ğŸ’": " ráº¥t tá»‘t ", "ğŸ’©": " ráº¥t tá»‘t ","ğŸ˜•": " ráº¥t chÃ¡n ", "ğŸ˜±": " ráº¥t chÃ¡n ", "ğŸ˜¸": " ráº¥t tá»‘t ",
            "ğŸ˜¾": " ráº¥t chÃ¡n ", "ğŸš«": "ráº¥t chÃ¡n ",  "ğŸ¤¬": " ráº¥t chÃ¡n ","ğŸ§š": " ráº¥t tá»‘t ", "ğŸ§¡": " ráº¥t tá»‘t ",'ğŸ¶':' ráº¥t tá»‘t ',
            'ğŸ‘': ' ráº¥t chÃ¡n ', 'ğŸ˜£': ' ráº¥t chÃ¡n ','âœ¨': ' ráº¥t tá»‘t ', 'â£': ' ráº¥t tá»‘t ','â˜€': ' ráº¥t tá»‘t ',
            'â™¥': ' ráº¥t tá»‘t ', 'ğŸ¤©': ' ráº¥t tá»‘t ', 'like': ' ráº¥t tá»‘t ', 'ğŸ’Œ': ' ráº¥t tá»‘t ',
            'ğŸ¤£': ' ráº¥t tá»‘t ', 'ğŸ–¤': ' ráº¥t tá»‘t ', 'ğŸ¤¤': ' ráº¥t tá»‘t ', ':(': ' ráº¥t chÃ¡n ', 'ğŸ˜¢': ' ráº¥t chÃ¡n ',
            'â¤': ' ráº¥t tá»‘t ', 'ğŸ˜': ' ráº¥t tá»‘t ', 'ğŸ˜˜': ' ráº¥t tá»‘t ', 'ğŸ˜ª': ' ráº¥t chÃ¡n ', 'ğŸ˜Š': ' ráº¥t tá»‘t ',
            '?': ' ? ', 'ğŸ˜': ' ráº¥t tá»‘t ', 'ğŸ’–': ' ráº¥t tá»‘t ', 'ğŸ˜Ÿ': ' ráº¥t chÃ¡n ', 'ğŸ˜­': ' ráº¥t chÃ¡n ',
            'ğŸ’¯': ' ráº¥t tá»‘t ', 'ğŸ’—': ' ráº¥t tá»‘t ', 'â™¡': ' ráº¥t tá»‘t ', 'ğŸ’œ': ' ráº¥t tá»‘t ', 'ğŸ¤—': ' ráº¥t tá»‘t ',
            '^^': ' ráº¥t tá»‘t ', 'ğŸ˜¨': ' ráº¥t chÃ¡n ', 'â˜º': ' ráº¥t tá»‘t ', 'ğŸ’‹': ' ráº¥t tá»‘t ', 'ğŸ‘Œ': ' ráº¥t tá»‘t ',
            'ğŸ˜–': ' ráº¥t chÃ¡n ', 'ğŸ˜€': ' ráº¥t tá»‘t ', ':((': ' ráº¥t chÃ¡n ', 'ğŸ˜¡': ' ráº¥t chÃ¡n ', 'ğŸ˜ ': ' ráº¥t chÃ¡n ',
            'ğŸ˜’': ' ráº¥t chÃ¡n ', 'ğŸ™‚': ' ráº¥t tá»‘t ', 'ğŸ˜': ' ráº¥t chÃ¡n ', 'ğŸ˜': ' ráº¥t tá»‘t ', 'ğŸ˜„': ' ráº¥t tá»‘t ',
            'ğŸ˜™': ' ráº¥t tá»‘t ', 'ğŸ˜¤': ' ráº¥t chÃ¡n ', 'ğŸ˜': ' ráº¥t tá»‘t ', 'ğŸ˜†': ' ráº¥t tá»‘t ', 'ğŸ’š': ' ráº¥t tá»‘t ',
            'âœŒ': ' ráº¥t tá»‘t ', 'ğŸ’•': ' ráº¥t tá»‘t ', 'ğŸ˜': ' ráº¥t chÃ¡n ', 'ğŸ˜“': ' ráº¥t chÃ¡n ', 'ï¸ğŸ†—ï¸': ' positive ',
            'ğŸ˜‰': ' ráº¥t tá»‘t ', 'ğŸ˜‚': ' ráº¥t tá»‘t ', ':v': '  ráº¥t tá»‘t ', '=))': '  ráº¥t tá»‘t ', 'ğŸ˜‹': ' ráº¥t tá»‘t ',
            'ğŸ’“': ' ráº¥t tá»‘t ', 'ğŸ˜': ' ráº¥t chÃ¡n ', ':3': ' ráº¥t tá»‘t ', 'ğŸ˜«': ' ráº¥t chÃ¡n ', 'ğŸ˜¥': ' ráº¥t chÃ¡n ',
            'ğŸ˜ƒ': ' ráº¥t tá»‘t ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' ráº¥t tá»‘t ',
            'ğŸ˜—': ' ráº¥t tá»‘t ', 'ğŸ¤”': ' ráº¥t chÃ¡n ', 'ğŸ˜‘': ' ráº¥t chÃ¡n ', 'ğŸ”¥': ' ráº¥t chÃ¡n ', 'ğŸ™': ' ráº¥t chÃ¡n ',
            'ğŸ†—': ' ráº¥t tá»‘t ', 'ğŸ˜»': ' ráº¥t tá»‘t ', 'ğŸ’™': ' ráº¥t tá»‘t ', 'ğŸ’Ÿ': ' ráº¥t tá»‘t ',
            'ğŸ˜š': ' ráº¥t tá»‘t ', 'âŒ': ' ráº¥t chÃ¡n ', 'ğŸ‘': ' ráº¥t tá»‘t ', ';)': ' ráº¥t tá»‘t ', '<3': ' ráº¥t tá»‘t ',
            'ğŸŒ': ' ráº¥t tá»‘t ',  'ğŸŒ·': ' ráº¥t tá»‘t ', 'ğŸŒ¸': ' ráº¥t tá»‘t ', 'ğŸŒº': ' ráº¥t tá»‘t ',
            'ğŸŒ¼': ' ráº¥t tá»‘t ', 'ğŸ“': ' ráº¥t tá»‘t ', 'ğŸ…': ' ráº¥t tá»‘t ', 'ğŸ¾': ' ráº¥t tá»‘t ', 'ğŸ‘‰': ' positive ',
            'ğŸ’': ' ráº¥t tá»‘t ', 'ğŸ’': ' ráº¥t tá»‘t ', 'ğŸ’¥': ' ráº¥t tá»‘t ', 'ğŸ’ª': ' ráº¥t tá»‘t ',
            'ğŸ’°': ' ráº¥t tá»‘t ',  'ğŸ˜‡': ' ráº¥t tá»‘t ', 'ğŸ˜›': ' ráº¥t tá»‘t ', 'ğŸ˜œ': ' ráº¥t tá»‘t ',
            'ğŸ™ƒ': ' ráº¥t tá»‘t ', 'ğŸ¤‘': ' ráº¥t tá»‘t ', 'ğŸ¤ª': ' ráº¥t tá»‘t ','â˜¹': ' ráº¥t chÃ¡n ',  'ğŸ’€': ' ráº¥t chÃ¡n ',
            'ğŸ˜”': ' ráº¥t chÃ¡n ', 'ğŸ˜§': ' ráº¥t chÃ¡n ', 'ğŸ˜©': ' ráº¥t chÃ¡n ', 'ğŸ˜°': ' ráº¥t chÃ¡n ', 'ğŸ˜³': ' ráº¥t chÃ¡n ',
            'ğŸ˜µ': ' ráº¥t chÃ¡n ', 'ğŸ˜¶': ' ráº¥t chÃ¡n ', 'ğŸ™': ' ráº¥t chÃ¡n ',
            #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
            ':))': '  ráº¥t tá»‘t ', ':)': ' ráº¥t tá»‘t ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
            'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',
            ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
            'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ',
            'kg ': u' khÃ´ng ','not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '"k ': u' khÃ´ng ',' kh ':u' khÃ´ng ','kÃ´':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
            'he he': ' ráº¥t tá»‘t ','hehe': ' ráº¥t tá»‘t ','hihi': ' ráº¥t tá»‘t ', 'haha': ' ráº¥t tá»‘t ', 'hjhj': ' ráº¥t tá»‘t ',
            ' lol ': ' ráº¥t chÃ¡n ',' cc ': ' ráº¥t chÃ¡n ','cute': u' dá»… thÆ°Æ¡ng ','huhu': ' ráº¥t chÃ¡n ', ' vs ': u' vá»›i ', 'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',
            ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',
            'Ä‘c': u' Ä‘Æ°á»£c ','authentic': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' ráº¥t tá»‘t ', 'store': u' cá»­a hÃ ng ',
            'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ','god': u' tá»‘t ','wel done':' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
            'sáº¥u': u' xáº¥u ','gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': u' bÃ¬nh thÆ°á»ng ',}
tunoi=[]
with open('TuNoi.csv', 'r', encoding='utf8') as csvFile:
    for row in csvFile:
        tunoi.append(row)
csvFile.close()
tunoi=[i.lower() for i in tunoi]
tunoi=[i.replace('\n',"") for i in tunoi]
tunoi=[i.replace('.','\.') for i in tunoi]
tunoi=[i for i in tunoi if i!=""]
def transform(sentences):
    sentences = sentences.lower()
    for key,value in replace_list.items():
        if key in sentences:
            sentences = re.sub(key,value, sentences)
    changelist=[]
    for i in tunoi:
        matchObj=re.search(i,sentences,re.L)
        if matchObj:
            t=matchObj.group()
            changelist.append(t)
        else:
            continue
    for i in changelist:
        sentences=sentences.replace(i,'<break>')
    sentences=sentences.split('<break>')
    output=[i for i in sentences if i!="" ]
    return output
print('day la dau vao dau tien',dauvaodautien)
dauvao_old=[]
with open('filedauvao.csv', 'w') as csvFile:
    for i in dauvaodautien:
        dauvao_old=transform(i)
        print(dauvao_old)
        writer = csv.writer(csvFile)
        writer.writerows(map(lambda x: [x], dauvao_old))
csvFile.close()
dauvao_new=[]
with open('filedauvao.csv', 'r', encoding='utf8') as file:
    for row in file:
        row = remove_noise(row)
        row = ViTokenizer.tokenize(row)
        dauvao_new.append(row)
csvFile.close()
print(dauvao_new)
one_gram = []
for i in viet_list:
    t = i.count(' ')
    if t == 0:
        one_gram.append(i)
viet_tat_dist = {}
with open('TXT_VietTat.txt', 'r', encoding='utf8') as file:
    for row in file:
        i = row.split(':')
        viet_tat_dist[i[0]] = re.sub('\n', '', i[1])
neg_list_fixed = []
neg_removed_set = []
i = 0
with open('daura.csv', 'w', encoding='utf8') as file1:
    for row in dauvao_new:
        i += 1
        for word in row.split(' '):
            if word.count('_') < 1:
                word = remove_noise(word)
                if word not in one_gram:
                    if word in viet_tat_dist:
                        row = row.replace(word, viet_tat_dist[word])
                    else:
                        neg_removed_set.append(word)
                        row = row.replace(word, '')
        print('replaced ' + str(i) + row)
        row = remove_noise(row)
        if len(row) != 0:
            file1.write(ViTokenizer.tokenize(row) + '\n')
        # file1.write(row + '\n')
        # file2.write(row)
